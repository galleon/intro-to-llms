{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ORVsQrWMkber",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df8213d2-48a5-4fbc-a20a-53e9517949f6"
      },
      "id": "ORVsQrWMkber",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/TDS/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU4T-YXuqmBf",
        "outputId": "ff937d30-5511-4aed-e988-0809fff12f39"
      },
      "id": "iU4T-YXuqmBf",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/TDS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "57a4d364-651f-43e0-852f-2c92283d7e39",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "57a4d364-651f-43e0-852f-2c92283d7e39"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# build on an old Intel MBP where Metal is not supported\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "27167078-379e-47a0-8e3b-445c7250b2a7",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "27167078-379e-47a0-8e3b-445c7250b2a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bdfda57-c4f3-417f-c3c0-49e97cda3e26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "model_name=\"mistral-7b-instruct-v0.1.Q5_K_M\"\n",
        "\n",
        "model = Llama(model_path=f\"{model_name}.gguf\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "eba256cc-d396-458d-bd26-bbbfeea7101a",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "eba256cc-d396-458d-bd26-bbbfeea7101a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31f8055-863c-4809-9a83-e1783ba8a203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        }
      ],
      "source": [
        "answer = model('In one sentence, what is a Large Language Model?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d31abda8-9391-41ed-9f8e-ddecd4299480",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "d31abda8-9391-41ed-9f8e-ddecd4299480",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d8303cd-c29a-4468-8af3-be4fa4478c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-f5a00e6f-58e6-4612-a3d9-b0f0c5e137ae', 'object': 'text_completion', 'created': 1698125260, 'model': 'mistral-7b-instruct-v0.1.Q5_K_M.gguf', 'choices': [{'text': '\\nAn AI system designed to process and generate large amounts of natural language text.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 12, 'completion_tokens': 16, 'total_tokens': 28}}\n"
          ]
        }
      ],
      "source": [
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4c2b9af6-1619-4f7c-b309-69f9b3802a38",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "4c2b9af6-1619-4f7c-b309-69f9b3802a38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6330ec47-69a2-4239-ef08-6e9ed38067d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "An AI system designed to process and generate large amounts of natural language text.\n"
          ]
        }
      ],
      "source": [
        "print(answer['choices'][0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZMKKsqrJzn3"
      },
      "id": "AZMKKsqrJzn3",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}